#!/usr/bin/env python

# modules

import argparse
import itertools
import gzip
import re

# user input arguments

def get_args():
    parser = argparse.ArgumentParser(description = "A program to demultiplex Illumina paired-end reads with dual matched indices")
    parser.add_argument("-a", "--read_one", help = "R1 fastq file", required = True)
    parser.add_argument("-b", "--read_two", help = "R2 fastq file", required = True)
    parser.add_argument("-c", "--read_three", help = "R3 fastq file", required = True)
    parser.add_argument("-d", "--read_four", help = "R4 fastq file", required = True)
    parser.add_argument("-i", "--indices_file", help = "plain txt file with list of indices used", required = True)
    return parser.parse_args()

args = get_args()

#--------------------------------------------------------------------------------------------------------------
# FUNCTIONS
#--------------------------------------------------------------------------------------------------------------

def create_outsuffix(filename):
    '''A function to build a suffix from Illumina standard output files that
    will later be concatenated to build demultiplexed output file names.
    Ex- in: 1294_S1_L008_R1_001.fastq.gz, out: L008_R1_001.fastq.
    Only works with file systems that use forward slashes for paths'''
    suffix = ""
    slash_split = re.split("/", filename) # paths are forward-slash delimited
    period_split = re.split("\.", slash_split[-1]) # takes input file name, splits by periods
    underscore_split = re.split("_", period_split[0]) # gets rid of .fastq.gz
    suffix += underscore_split[2] + "_" # building the suffix with only desired elements
    suffix += underscore_split[3] + "_"
    suffix += underscore_split[4]
    suffix += ".fastq"
    return suffix

def ouput_names(identifiers, fw_suffix, rv_suffix):
    '''A function that generates output file names for reads with matched 
    indices. Generates one R1 file and one R4 file. Takes column data extracted
    from indexes file and suffixes generated by create_outsuffix function'''
    fw_file = "./"
    rv_file = "./"
    for item in identifiers: # gathering index and sample data, concatenating to file names
        fw_file += item + "_"
        rv_file += item + "_"
    fw_file += fw_suffix # finally, concatenating suffix generated by function to file names
    rv_file += rv_suffix
    return [fw_file, rv_file]

def reverse_complement(sequence, nucleotide_dict):
    '''A function to reverse complement a DNA sequence using a 
    global revcomp_dict dictionary, unknown bases remain N'''
    rev_seq = ""
    for char in sequence:
        rev_seq += nucleotide_dict[char]
    rev_seq = rev_seq[::-1]
    return rev_seq

def quality_check(qscore_seq):
    '''A function that takes a qscore ASCII sequence, converts to integers. 
    Returns True if every base has a score of Q30 or above, False if not'''
    pass_test = True
    for char in qscore_seq:
        score = ord(char) - 33
        if score < 30:
            pass_test = False
            break
    return pass_test

def insert_indices(read_list, index1, index2):
    '''A function to format the header lines of fastq records to contain 
    their respective index sequences separated by a hyphen at the end of the line'''
    header1 = read_list[0]
    combined_indices = ":" + index1 + "-" + index2 
    header1 = header1 + combined_indices
    read_list[0] = header1
    return read_list

#-----------------------------------------------------------------------------------------------------------
# MAIN
#-----------------------------------------------------------------------------------------------------------

# Generating output file suffixes
r1_suffix = create_outsuffix(args.read_one)
r4_suffix = create_outsuffix(args.read_four)

# create dict with known indices as keys and list of associated output files as values
# Example: index_seq : [samp_grp_treat_index_lane_R1_001.fastq, samp_grp_treat_index_lane_R4_001.fastq]

known_indices = {}

# this dict will be used for the statistics output file
sample_data = {}

with open(args.indices_file, "r") as fh:
    lin_num = 0
    for line in fh:
        if lin_num > 0: # skip header line
            line = line.strip("\n")
            col_objects = re.split("\t", line)
            index_info = col_objects[0:4] # the sample, group, treatment, and name associated with each index
            out_files = ouput_names(index_info, r1_suffix, r4_suffix)
            index_seq = col_objects[4] # just the index sequence
            known_indices[index_seq] = out_files
            sample_data[index_seq] = index_info
        lin_num += 1

# viable index pair permutations dictionary (pair:count), initialize count to zero

permutations_dict = {}

for perm in itertools.product(known_indices.keys(), repeat = 2):
    permutations_dict[perm] = 0

# open 4 input files, 48 matched output files, 2 hopped out files, 
# 2 unknown/lowq out files, 1 stats file

input_files = [args.read_one, args.read_two, args.read_three, args.read_four]

additional_files_strings = [
    "./index_hopped_R1.fastq", 
    "./index_hopped_R4.fastq", 
    "./unknown_lowq_R1.fastq", 
    "./unknown_lowq_R4.fastq", 
    "./demultiplex_stats.txt"
]

# input files

unzipped_files = []
for infile in input_files:
    unzfile = gzip.open(infile, "rt")
    unzipped_files.append(unzfile)

# unknown/lowq, index hopped, and stats files

additional_files = []
for add_file in additional_files_strings:
    opened_addfile = open(add_file, "w")
    additional_files.append(opened_addfile)

# 48 index matched output files

for key in known_indices:
    for i in range(0, 2):
        known_indices[key][i] = open(known_indices[key][i], "w")

# initiate counters

match_cnt = 0
hopped_cnt = 0
unknown_cnt = 0
lowq_cnt = 0

line_ctr = 1
record_ctr = 0


# dictionary for reverse complementing

revcomp_dict = {
    "A" : "T", 
    "T" : "A",
    "G" : "C",
    "C" : "G",
    "N" : "N"
}

# initializing current record keepers
current_r1 = []
current_r2 = []
current_r3 = []
current_r4 = []

# iterating through the four input files in lock-step

for line_group in zip(unzipped_files[0], unzipped_files[1], unzipped_files[2], unzipped_files[3]):
    line1 = line_group[0] #unpacking the zip object used to synchronize line iteration
    line2 = line_group[1]
    line3 = line_group[2]
    line4 = line_group[3]
    line1 = line1.strip("\n")
    line2 = line2.strip("\n")
    line3 = line3.strip("\n")
    line4 = line4.strip("\n")
    if line_ctr // 4 == record_ctr: # still in a record? keep appending lines
        current_r1.append(line1)
        current_r2.append(line2)
        current_r3.append(line3)
        current_r4.append(line4)
    else: # At the last line of a record, stop, and do everything pertaining to this record before continuing
        current_r1.append(line1) # captures the last line
        current_r2.append(line2)
        current_r3.append(line3)
        current_r4.append(line4)
        working_index1 = current_r2[1]
        working_index2 = reverse_complement(current_r3[1], revcomp_dict) # reverse complement R3 
        current_r1 = insert_indices(current_r1, working_index1, working_index2) #formatting header lines
        current_r4 = insert_indices(current_r4, working_index1, working_index2)
        if working_index1 not in known_indices or working_index2 not in known_indices: # parsing out records w unknown indices
            for i in current_r1:
                additional_files[2].write(i + "\n")
            for j in current_r4:
                additional_files[3].write(j + "\n")
            unknown_cnt += 1
        else: # known indices, now to parse out records with low quality indices
            r2_quality = current_r2[3]
            r3_quality = current_r3[3]
            if (quality_check(r2_quality) == False) or (quality_check(r3_quality) == False):
                for i in current_r1:
                    additional_files[2].write(i + "\n")
                for j in current_r4:
                    additional_files[3].write(j + "\n")
                lowq_cnt += 1
            else: # known, high quality, now to see if matched or hopped
                index_tuple = (working_index1, working_index2) # tuple for incrementing permutation counter
                if working_index1 == working_index2:
                    for i in current_r1:
                        known_indices[working_index1][0].write(i + "\n")
                    for j in current_r4:
                        known_indices[working_index1][1].write(j + "\n")
                    permutations_dict[index_tuple] += 1
                    match_cnt += 1
                else: # index hopped
                    for i in current_r1:
                        additional_files[0].write(i + "\n")
                    for j in current_r4:
                        additional_files[1].write(j + "\n")
                    permutations_dict[index_tuple] += 1
                    hopped_cnt += 1
        record_ctr += 1
        current_r1 = []
        current_r2 = []
        current_r3 = []
        current_r4 = [] # empty the current record lists to begin the next one
    line_ctr += 1

# Calculating percentages

match_percent = round((match_cnt/record_ctr) * 100, 2)
hopped_percent = round((hopped_cnt/record_ctr) * 100, 2)
unknown_percent = round((unknown_cnt/record_ctr) * 100, 2)
lowq_percent = round((lowq_cnt/record_ctr) * 100, 2)

# Formatting and outputting the statistics

additional_files[4].write("Total number of read pairs" + "\t" + str(record_ctr) + "\n")
additional_files[4].write("\n")
additional_files[4].write("Category" + "\t" + "Count" + "\t" + "Percent of Total Read Pairs" + "\n")
additional_files[4].write("Matching pairs" + "\t" + str(match_cnt) + "\t" + str(match_percent) + "\n")
additional_files[4].write("Index hopped pairs" + "\t" + str(hopped_cnt) + "\t" + str(hopped_percent) + "\n")
additional_files[4].write("One or more unknown index" + "\t" + str(unknown_cnt) + "\t" + str(unknown_percent) + "\n")
additional_files[4].write("One or more low quality index" + "\t" + str(lowq_cnt) + "\t" + str(lowq_percent) + "\n")
additional_files[4].write("\n")

additional_files[4].write("Sample" + "\t" "Matched Index pair" + "\t" + "No. of read pairs" + "\t" + "Percent of total matched read pairs" + "\n")
for key in permutations_dict.keys():
    if key[0] == key[1]:
        perm_percent = round((permutations_dict[key] / match_cnt) *100, 2)
        additional_files[4].write(sample_data[key[0]][0] + "\t" + key[0] + "-" + key[1] + "\t" + str(permutations_dict[key]) + "\t" + str(perm_percent) + "\n")

additional_files[4].write("\n")

additional_files[4].write("Hopped Index pair" + "\t" + "No. of read pairs" + "\n")
for key in permutations_dict.keys():
    if key[0] != key[1]:
        additional_files[4].write(key[0] + "-" + key[1] + "\t" + str(permutations_dict[key]) + "\n")

# close all files

for unzfile in unzipped_files:
    unzfile.close()

for add_file in additional_files:
    add_file.close()

for key in known_indices:
    for open_file in known_indices[key]:
        open_file.close()
